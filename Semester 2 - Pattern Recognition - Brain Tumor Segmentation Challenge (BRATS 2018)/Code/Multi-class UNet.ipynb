{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Test Kaggle Directory"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/vs-brats2018/miccai_brats_2018_data_training\"))\nprint(os.listdir(\"/kaggle\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls /kaggle/input","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Delete Directories(Optional)\n*   For deleing the generated database."},{"metadata":{"trusted":true},"cell_type":"code","source":"if (True):\n    import os\n    import sys\n    import shutil\n\n    try:\n        shutil.rmtree(\"../BraTs_Training_p508_npy/\")\n    except OSError as e:\n        print (\"Error: %s - %s.\" % (e.filename, e.strerror))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport nibabel as nib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\n\nIMG_WIDTH = 240\nIMG_HEIGHT = 240\nIMG_CHANNELS = 4\nHW = 100\n\nenable_original_axial = True;\nenable_original_sagittal = True;\nenable_original_coronal = True;\n\nTRAIN_PATH = \"../input/vs-brats2018/miccai_brats_2018_data_training\"\n\nAXIAL_DATA_PATH = \"../BraTs_Training_p508_npy/original/axial/Data\"\nAXIAL_LABEL_PATH = \"../BraTs_Training_p508_npy/original/axial/Label\"\n\nSAGITTAL_DATA_PATH = \"../BraTs_Training_p508_npy/original/sagittal/Data\"\nSAGITTAL_LABEL_PATH = \"../BraTs_Training_p508_npy/original/sagittal/Label\"\n\nCORONAL_DATA_PATH = \"../BraTs_Training_p508_npy/original/coronal/Data\"\nCORONAL_LABEL_PATH = \"../BraTs_Training_p508_npy/original/coronal/Label\"\n\n\nimport matplotlib.image as mpimg\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\nimport numpy.ma as ma\n\ndirs_all = os.listdir(TRAIN_PATH)\ndirs_all.sort()\ndirs_train = dirs_all[:2]\ntraining_data = []\nlabel_data = []\n\n\ndef normalize_image(im):\n  im_mask = (im > 0);\n  im_compressed_region = ma.array(im,mask=~im_mask).compressed()\n  #print(im_compressed_region.size)\n  \n  #plt.imshow(im_mask)\n  #plt.show()\n  #plt.imshow(im)\n  #plt.show()\n  if (im_compressed_region.size == 0):\n    return im\n  else:\n    avg = np.mean(im_compressed_region,dtype='float32')\n    std = np.std(im_compressed_region,dtype='float32')\n    return (ma.array(im,mask=~im_mask)-avg)/std\n\naxial_data_path = [];\naxial_label_path = [];\n\nsagittal_data_path = [];\nsagittal_label_path = [];\n\ncoronal_data_path = [];\ncoronal_label_path = [];\nfor nf, folder in enumerate(dirs_train):\n  print(\"Processing Images...\")\n  HLG_PATH = os.path.join(TRAIN_PATH,folder)\n  hlg_all = os.listdir(HLG_PATH)\n  for n, file in enumerate(hlg_all):\n    print(\"Image: \"+ str(nf) + \", \" + str(n))\n    flair = nib.load(os.path.join(HLG_PATH, file, file + '_flair.nii.gz')).get_data()\n    t1 = nib.load(os.path.join(HLG_PATH, file, file + '_t1.nii.gz')).get_data()\n    t1ce = nib.load(os.path.join(HLG_PATH, file, file + '_t1ce.nii.gz')).get_data()\n    t2 = nib.load(os.path.join(HLG_PATH, file, file + '_t2.nii.gz')).get_data()\n    label = nib.load(os.path.join(HLG_PATH, file, file + '_seg.nii.gz')).get_data()\n    \n    if(enable_original_axial):\n        print(\"Axial Data:\")    \n        folder_directory = os.path.join(AXIAL_DATA_PATH,folder,file)\n        if not os.path.exists(folder_directory):\n          os.makedirs(folder_directory)\n          for i in range(155):\n            if(np.mean(flair[:,:,i]+t1[:,:,i]+t1ce[:,:,i]+t2[:,:,i])!=0):\n                data_patch_axial = np.zeros([240,240,4],dtype='float32')\n\n                data_patch_axial[:,:,0] = normalize_image(flair[:,:,i])\n                data_patch_axial[:,:,1] = normalize_image(t1[:,:,i])\n                data_patch_axial[:,:,2] = normalize_image(t1ce[:,:,i])\n                data_patch_axial[:,:,3] = normalize_image(t2[:,:,i])\n\n                np.savez_compressed(os.path.join(folder_directory,'pao'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz'),data_patch_axial)\n                axial_data_path.append([nf,n,os.path.join(folder_directory,'pao'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz')]);  \n                \n\n        print(\"Axial Label:\")\n        folder_directory = os.path.join(AXIAL_LABEL_PATH,folder,file)\n        if not os.path.exists(folder_directory):\n          os.makedirs(folder_directory)\n\n          for i in range(155):\n            if(np.mean(flair[:,:,i]+t1[:,:,i]+t1ce[:,:,i]+t2[:,:,i])!=0):\n                label_patch_axial = np.zeros([240,240,4],dtype='int')\n\n                label_patch_axial[:,:,0] = (label[:,:,i]==0).astype(int)\n                label_patch_axial[:,:,1] = (label[:,:,i]==1).astype(int)\n                label_patch_axial[:,:,2] = (label[:,:,i]==2).astype(int)\n                label_patch_axial[:,:,3] = (label[:,:,i]==4).astype(int)\n\n                np.savez_compressed(os.path.join(folder_directory,'pao'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz'),label_patch_axial)\n                axial_label_path.append([nf,n,os.path.join(folder_directory,'pao'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz')]);\n\n    ##############################################################################################################################################################################################\n    if(enable_original_sagittal):\n        print(\"Sagittal Data:\")\n        folder_directory = os.path.join(SAGITTAL_DATA_PATH,folder,file)\n        if not os.path.exists(folder_directory):\n          os.makedirs(folder_directory)\n          for i in range(240):\n            if(np.mean(flair[i,:,:]+t1[i,:,:]+t1ce[i,:,:]+t2[i,:,:])!=0):\n                data_patch_sagittal = np.zeros([240,240,4],dtype='float32')\n\n                data_patch_sagittal[:,41:196,0] = normalize_image(flair[i,:,:])\n                data_patch_sagittal[:,41:196,1] = normalize_image(t1[i,:,:])\n                data_patch_sagittal[:,41:196,2] = normalize_image(t1ce[i,:,:])\n                data_patch_sagittal[:,41:196,3] = normalize_image(t2[i,:,:])\n\n                np.savez_compressed(os.path.join(folder_directory,'pso'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz'),data_patch_sagittal)\n                sagittal_data_path.append([nf,n,os.path.join(folder_directory,'pso'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz')]);\n\n        print(\"Sagittal Label:\")\n        folder_directory = os.path.join(SAGITTAL_LABEL_PATH,folder,file)\n        if not os.path.exists(folder_directory):\n          os.makedirs(folder_directory)\n          for i in range(240):\n            if(np.mean(flair[i,:,:]+t1[i,:,:]+t1ce[i,:,:]+t2[i,:,:])!=0):\n                label_patch_sagittal = np.zeros([240,240,4],dtype='int')\n                \n                label_patch_sagittal[:,:,0] = label_patch_sagittal[:,:,0] + 1\n                label_patch_sagittal[:,41:196,0] = (label[i,:,:]==0).astype(int)\n                label_patch_sagittal[:,41:196,1] = (label[i,:,:]==1).astype(int)\n                label_patch_sagittal[:,41:196,2] = (label[i,:,:]==2).astype(int)\n                label_patch_sagittal[:,41:196,3] = (label[i,:,:]==4).astype(int)\n                \n                np.savez_compressed(os.path.join(folder_directory,'pso'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz'),label_patch_sagittal)\n                sagittal_label_path.append([nf,n,os.path.join(folder_directory,'pso'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz')]);\n\n    ##############################################################################################################################################################################################\n    if(enable_original_coronal):\n        print(\"Coronal Data:\")\n        folder_directory = os.path.join(CORONAL_DATA_PATH,folder,file)\n        if not os.path.exists(folder_directory):\n          os.makedirs(folder_directory)\n          for i in range(240):\n            if(np.mean(flair[:,i,:]+t1[:,i,:]+t1ce[:,i,:]+t2[:,i,:])!=0):\n                data_patch_coronal = np.zeros([240,240,4],dtype='float32')\n\n                data_patch_coronal[:,41:196,0] = normalize_image(flair[:,i,:])\n                data_patch_coronal[:,41:196,1] = normalize_image(t1[:,i,:])\n                data_patch_coronal[:,41:196,2] = normalize_image(t1ce[:,i,:])\n                data_patch_coronal[:,41:196,3] = normalize_image(t2[:,i,:])\n\n                np.savez_compressed(os.path.join(folder_directory,'pco'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz'),data_patch_coronal)\n                coronal_data_path.append([nf,n,os.path.join(folder_directory,'pco'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz')]);\n\n        print(\"Coronal Label:\")          \n        folder_directory = os.path.join(CORONAL_LABEL_PATH,folder,file)\n        if not os.path.exists(folder_directory):\n          os.makedirs(folder_directory)\n          for i in range(240):\n            if(np.mean(flair[:,i,:]+t1[:,i,:]+t1ce[:,i,:]+t2[:,i,:])!=0):\n                label_patch_coronal = np.zeros([240,240,4],dtype='int')\n                \n                label_patch_coronal[:,:,0] = label_patch_coronal[:,:,0] + 1\n                label_patch_coronal[:,41:196,0] = (label[:,i,:]==0).astype(int)\n                label_patch_coronal[:,41:196,1] = (label[:,i,:]==1).astype(int)\n                label_patch_coronal[:,41:196,2] = (label[:,i,:]==2).astype(int)\n                label_patch_coronal[:,41:196,3] = (label[:,i,:]==4).astype(int)\n\n                np.savez_compressed(os.path.join(folder_directory,'pco'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz'),label_patch_coronal)\n                coronal_label_path.append([nf,n,os.path.join(folder_directory,'pco'+ '_' + str(nf) + '_' + str(n) + '_' + str(i) + '.npz')]);\n\nif(enable_original_axial):\n    np.savez_compressed(os.path.join('../BraTs_Training_p508_npy/axial_data_path'+ '.npz'),axial_data_path)    \n    np.savez_compressed(os.path.join('../BraTs_Training_p508_npy/axial_label_path'+ '.npz'),axial_label_path)\nif(enable_original_sagittal):\n    np.savez_compressed(os.path.join('../BraTs_Training_p508_npy/sagittal_data_path'+ '.npz'),sagittal_data_path)    \n    np.savez_compressed(os.path.join('../BraTs_Training_p508_npy/sagittal_label_path'+ '.npz'),sagittal_label_path)\nif(enable_original_coronal):\n    np.savez_compressed(os.path.join('../BraTs_Training_p508_npy/coronal_data_path'+ '.npz'),coronal_data_path)    \n    np.savez_compressed(os.path.join('../BraTs_Training_p508_npy/coronal_label_path'+ '.npz'),coronal_label_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Show generated numpy data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#del axial_data_path\n#del axial_label_path\n#del data_patch_axial\n#del label_patch_axial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for paient_idx in range(len(dirs_axial_data_decompress)):\n#    if (dirs_axial_data_decompress[paient_idx][2].find('Brats18_TCIA10_266_1') != -1): \n#        print (dirs_axial_data_decompress[paient_idx][0])\n#        print (dirs_axial_data_decompress[paient_idx][1])\n    #if(dirs_axial_data_decompress[paient_idx][0] == str(0)):\n        #if(dirs_axial_data_decompress[paient_idx][1] == str(5)):\n            #print(dirs_axial_data_decompress[paient_idx][2])\n#print(dirs_axial_data_decompress[0][0])\n#print(dirs_axial_data_decompress[0][1])\n#print(dirs_axial_data_decompress[0][2])\n\n\nimport random\n\nTesting_patients_index = [[0,36],[0,128],[0,68],[0,17],[0,32],[0,180],[0,54],[1,0],[1,7],[1,57]]\n\nLGG_testing_idx  = [0,7,57]\nHGG_testing_idx  = [36,128,68,17,32,180,54]\n\nprint(\"Start: Testing\")\nprint(len(LGG_testing_idx))\nprint(len(HGG_testing_idx))\nLGG_non_testing_idx = [x for x in range(0,75) if x not in LGG_testing_idx]\nHGG_non_testing_idx = [x for x in range(0,210) if x not in HGG_testing_idx]\n\nLGG_validation_idx = LGG_non_testing_idx[:8]\nHGG_validation_idx = HGG_non_testing_idx[:22]\n\nprint(\"Start: Validation\")\nprint(len(LGG_validation_idx))\nprint(len(HGG_validation_idx))\n\nLGG_training_idx = LGG_non_testing_idx[8:]\nHGG_training_idx = HGG_non_testing_idx[22:]\nprint(\"Start: Training\")\nprint(len(LGG_training_idx))\nprint(len(HGG_training_idx))\n\n\nif(enable_original_axial):\n    dirs_axial_data = np.load('../BraTs_Training_p508_npy/axial_data_path'+ '.npz')\n    dirs_axial_label = np.load('../BraTs_Training_p508_npy/axial_label_path'+ '.npz')\n    dirs_axial_data_decompress = dirs_axial_data['arr_0']\n    dirs_axial_label_decompress = dirs_axial_label['arr_0']\n    dirs_axial_data_training = [];\n    dirs_axial_label_training = [];\n    dirs_axial_data_validation = [];\n    dirs_axial_label_validation = [];\n    dirs_axial_data_testing = [];\n    dirs_axial_label_testing = [];\n    hgg_len = 0\n    lgg_len =0\n    hgg_len_val = 0\n    lgg_len_val =0\n    hgg_len_train = 0\n    lgg_len_train =0\n    hgg_len_test = 0\n    lgg_len_test =0\n    \n    for dirs_idx in range(len(dirs_axial_data_decompress)):\n        if(dirs_axial_data_decompress[dirs_idx][0] == str(0)):\n            #hgg_len = hgg_len+1\n            for idx in range(len(HGG_testing_idx)):\n                if(dirs_axial_data_decompress[dirs_idx][1]==str(HGG_testing_idx[idx])):\n                    #hgg_len_test = hgg_len_test + 1\n                    #print(dirs_axial_data_decompress[dirs_idx][1])\n                    dirs_axial_data_testing.append(dirs_axial_data_decompress[dirs_idx][2])\n                    dirs_axial_label_testing.append(dirs_axial_label_decompress[dirs_idx][2])\n                    \n            for idx in range(len(HGG_validation_idx)):\n                if(dirs_axial_data_decompress[dirs_idx][1]==str(HGG_validation_idx[idx])):\n                    #hgg_len_val = hgg_len_val + 1\n                    dirs_axial_data_validation.append(dirs_axial_data_decompress[dirs_idx][2])\n                    dirs_axial_label_validation.append(dirs_axial_label_decompress[dirs_idx][2])\n                    \n            for idx in range(len(HGG_training_idx)):\n                if(dirs_axial_data_decompress[dirs_idx][1]==str(HGG_training_idx[idx])):\n                    #hgg_len_train = hgg_len_train + 1\n                    dirs_axial_data_training.append(dirs_axial_data_decompress[dirs_idx][2])\n                    dirs_axial_label_training.append(dirs_axial_label_decompress[dirs_idx][2])\n            \n        if(dirs_axial_data_decompress[dirs_idx][0] == str(1)):\n            #lgg_len = lgg_len+1\n            for idx in range(len(LGG_testing_idx)):\n                if(dirs_axial_data_decompress[dirs_idx][1]==str(LGG_testing_idx[idx])):\n                    #lgg_len_test = lgg_len_test + 1\n                    dirs_axial_data_testing.append(dirs_axial_data_decompress[dirs_idx][2])\n                    dirs_axial_label_testing.append(dirs_axial_label_decompress[dirs_idx][2])\n                   \n            for idx in range(len(LGG_validation_idx)):\n                if(dirs_axial_data_decompress[dirs_idx][1]==str(LGG_validation_idx[idx])):\n                    #lgg_len_val = lgg_len_val + 1\n                    dirs_axial_data_validation.append(dirs_axial_data_decompress[dirs_idx][2])\n                    dirs_axial_label_validation.append(dirs_axial_label_decompress[dirs_idx][2])\n                   \n            for idx in range(len(LGG_training_idx)):\n                if(dirs_axial_data_decompress[dirs_idx][1]==str(LGG_training_idx[idx])):\n                    #lgg_len_train = lgg_len_train + 1\n                    dirs_axial_data_training.append(dirs_axial_data_decompress[dirs_idx][2])\n                    dirs_axial_label_training.append(dirs_axial_label_decompress[dirs_idx][2])\n    \n    print(len(dirs_axial_data_decompress))\n    print(len(dirs_axial_data_testing))\n    print(len(dirs_axial_data_validation))\n    print(len(dirs_axial_data_training))\n    \"\"\"\n    print(\"HGG TOTAL\")\n    print(hgg_len)\n    print(hgg_len_test)\n    print(hgg_len_val)\n    print(hgg_len_train)\n\n    print(\"LGG TOTAL\")\n    print(lgg_len)\n    print(lgg_len_test)\n    print(lgg_len_val)\n    print(lgg_len_train)\n    \"\"\"\nif(enable_original_sagittal):\n    dirs_sagittal_data = np.load('../BraTs_Training_p508_npy/sagittal_data_path'+ '.npz')\n    dirs_sagittal_label = np.load('../BraTs_Training_p508_npy/sagittal_label_path'+ '.npz')\n    dirs_sagittal_data_decompress = dirs_sagittal_data['arr_0']\n    dirs_sagittal_label_decompress = dirs_sagittal_label['arr_0']\n    dirs_sagittal_data_training = [];\n    dirs_sagittal_label_training = [];\n    dirs_sagittal_data_validation = [];\n    dirs_sagittal_label_validation = [];\n    dirs_sagittal_data_testing = [];\n    dirs_sagittal_label_testing = [];\n    \n    for dirs_idx in range(len(dirs_sagittal_data_decompress)):\n\n        if(dirs_sagittal_data_decompress[dirs_idx][0] == str(0)):\n            for idx in range(len(HGG_testing_idx)):\n                if(dirs_sagittal_data_decompress[dirs_idx][1]==str(HGG_testing_idx[idx])):\n                    dirs_sagittal_data_testing.append(dirs_sagittal_data_decompress[dirs_idx][2])\n                    dirs_sagittal_label_testing.append(dirs_sagittal_label_decompress[dirs_idx][2])\n\n            for idx in range(len(HGG_validation_idx)):\n                if(dirs_sagittal_data_decompress[dirs_idx][1]==str(HGG_validation_idx[idx])):\n                    dirs_sagittal_data_validation.append(dirs_sagittal_data_decompress[dirs_idx][2])\n                    dirs_sagittal_label_validation.append(dirs_sagittal_label_decompress[dirs_idx][2])\n\n            for idx in range(len(HGG_training_idx)):\n                if(dirs_sagittal_data_decompress[dirs_idx][1]==str(HGG_training_idx[idx])):\n                    dirs_sagittal_data_training.append(dirs_sagittal_data_decompress[dirs_idx][2])\n                    dirs_sagittal_label_training.append(dirs_sagittal_label_decompress[dirs_idx][2])\n\n        if(dirs_sagittal_data_decompress[dirs_idx][0] == str(1)):\n            for idx in range(len(LGG_testing_idx)):\n                if(dirs_sagittal_data_decompress[dirs_idx][1]==str(LGG_testing_idx[idx])):\n                    dirs_sagittal_data_testing.append(dirs_sagittal_data_decompress[dirs_idx][2])\n                    dirs_sagittal_label_testing.append(dirs_sagittal_label_decompress[dirs_idx][2])\n\n            for idx in range(len(LGG_validation_idx)):\n                if(dirs_sagittal_data_decompress[dirs_idx][1]==str(LGG_validation_idx[idx])):\n                    dirs_sagittal_data_validation.append(dirs_sagittal_data_decompress[dirs_idx][2])\n                    dirs_sagittal_label_validation.append(dirs_sagittal_label_decompress[dirs_idx][2])\n\n            for idx in range(len(LGG_training_idx)):\n                if(dirs_sagittal_data_decompress[dirs_idx][1]==str(LGG_training_idx[idx])):\n                    dirs_sagittal_data_training.append(dirs_sagittal_data_decompress[dirs_idx][2])\n                    dirs_sagittal_label_training.append(dirs_sagittal_label_decompress[dirs_idx][2])\n    \n    print(len(dirs_sagittal_data_decompress))\n    print(len(dirs_sagittal_data_testing))\n    print(len(dirs_sagittal_data_validation))\n    print(len(dirs_sagittal_data_training))\n                    \nif(enable_original_coronal):\n    dirs_coronal_data = np.load('../BraTs_Training_p508_npy/coronal_data_path'+ '.npz')\n    dirs_coronal_label = np.load('../BraTs_Training_p508_npy/coronal_label_path'+ '.npz')\n    dirs_coronal_data_decompress = dirs_coronal_data['arr_0']\n    dirs_coronal_label_decompress = dirs_coronal_label['arr_0']\n    dirs_coronal_data_training = [];\n    dirs_coronal_label_training = [];\n    dirs_coronal_data_validation = [];\n    dirs_coronal_label_validation = [];\n    dirs_coronal_data_testing = [];\n    dirs_coronal_label_testing = [];\n\n    for dirs_idx in range(len(dirs_coronal_data_decompress)):\n        if(dirs_coronal_data_decompress[dirs_idx][0] == str(0)):\n            for idx in range(len(HGG_testing_idx)):\n                if(dirs_coronal_data_decompress[dirs_idx][1]==str(HGG_testing_idx[idx])):\n                    dirs_coronal_data_testing.append(dirs_coronal_data_decompress[dirs_idx][2])\n                    dirs_coronal_label_testing.append(dirs_coronal_label_decompress[dirs_idx][2])\n\n            for idx in range(len(HGG_validation_idx)):\n                if(dirs_coronal_data_decompress[dirs_idx][1]==str(HGG_validation_idx[idx])):\n                    dirs_coronal_data_validation.append(dirs_coronal_data_decompress[dirs_idx][2])\n                    dirs_coronal_label_validation.append(dirs_coronal_label_decompress[dirs_idx][2])\n\n            for idx in range(len(HGG_training_idx)):\n                if(dirs_coronal_data_decompress[dirs_idx][1]==str(HGG_training_idx[idx])):\n                    dirs_coronal_data_training.append(dirs_coronal_data_decompress[dirs_idx][2])\n                    dirs_coronal_label_training.append(dirs_coronal_label_decompress[dirs_idx][2])\n\n        if(dirs_coronal_data_decompress[dirs_idx][0] == str(1)):\n            for idx in range(len(LGG_testing_idx)):\n                if(dirs_coronal_data_decompress[dirs_idx][1]==str(LGG_testing_idx[idx])):\n                    dirs_coronal_data_testing.append(dirs_coronal_data_decompress[dirs_idx][2])\n                    dirs_coronal_label_testing.append(dirs_coronal_label_decompress[dirs_idx][2])\n\n            for idx in range(len(LGG_validation_idx)):\n                if(dirs_coronal_data_decompress[dirs_idx][1]==str(LGG_validation_idx[idx])):\n                    dirs_coronal_data_validation.append(dirs_coronal_data_decompress[dirs_idx][2])\n                    dirs_coronal_label_validation.append(dirs_coronal_label_decompress[dirs_idx][2])\n\n            for idx in range(len(LGG_training_idx)):\n                if(dirs_coronal_data_decompress[dirs_idx][1]==str(LGG_training_idx[idx])):\n                    dirs_coronal_data_training.append(dirs_coronal_data_decompress[dirs_idx][2])\n                    dirs_coronal_label_training.append(dirs_coronal_label_decompress[dirs_idx][2])\n                                  \n    print(len(dirs_coronal_data_decompress))\n    print(len(dirs_coronal_data_testing))\n    print(len(dirs_coronal_data_validation))\n    print(len(dirs_coronal_data_training))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\n\nif(False):\n    for i in range(len(dirs_axial_data_training)):\n        print(i)\n        dummy_data = np.load(dirs_axial_data_training[i])\n        dummy_data_decompress = dummy_data['arr_0']\n        dummy_label = np.load(dirs_axial_label_training[i])\n        dummy_label_decompress = dummy_label['arr_0']\n        print(dummy_data_decompress.shape)\n        plt.figure(figsize=(15,8))\n        plt.subplot(2, 4, 1)\n        plt.imshow(dummy_data_decompress[:,:,0],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 2)\n        plt.imshow(dummy_data_decompress[:,:,1],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 3)\n        plt.imshow(dummy_data_decompress[:,:,2],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 4)\n        plt.imshow(dummy_data_decompress[:,:,3],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 5)\n        plt.imshow(dummy_label_decompress[:,:,0],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 6)\n        plt.imshow(dummy_label_decompress[:,:,1],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 7)\n        plt.imshow(dummy_label_decompress[:,:,2],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 8)\n        plt.imshow(dummy_label_decompress[:,:,3],cmap='gray') # BGR->RGB for plotting\n        plt.show()\n        \n\nif(False):\n    for i in range(len(dirs_sagittal_data_training)):\n        print(i)\n        dummy_data = np.load(dirs_sagittal_data_training[i])\n        dummy_data_decompress = dummy_data['arr_0']\n        dummy_label = np.load(dirs_sagittal_label_training[i])\n        dummy_label_decompress = dummy_label['arr_0']\n        print(dummy_data_decompress.shape)\n        plt.figure(figsize=(15,8))\n        plt.subplot(2, 4, 1)\n        plt.imshow(dummy_data_decompress[:,:,0],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 2)\n        plt.imshow(dummy_data_decompress[:,:,1],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 3)\n        plt.imshow(dummy_data_decompress[:,:,2],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 4)\n        plt.imshow(dummy_data_decompress[:,:,3],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 5)\n        plt.imshow(dummy_label_decompress[:,:,0],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 6)\n        plt.imshow(dummy_label_decompress[:,:,1],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 7)\n        plt.imshow(dummy_label_decompress[:,:,2],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 8)\n        plt.imshow(dummy_label_decompress[:,:,3],cmap='gray') # BGR->RGB for plotting\n        plt.show()\n\nif(False):\n    for i in range(len(dirs_coronal_data_training)):\n        print(i)\n        dummy_data = np.load(dirs_coronal_data_training[i,])\n        dummy_data_decompress = dummy_data['arr_0']\n        dummy_label = np.load(dirs_coronal_label_training[i,])\n        dummy_label_decompress = dummy_label['arr_0']\n        print(dummy_data_decompress.shape)\n        plt.figure(figsize=(15,8))\n        plt.subplot(2, 4, 1)\n        plt.imshow(dummy_data_decompress[:,:,0],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 2)\n        plt.imshow(dummy_data_decompress[:,:,1],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 3)\n        plt.imshow(dummy_data_decompress[:,:,2],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 4)\n        plt.imshow(dummy_data_decompress[:,:,3],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 5)\n        plt.imshow(dummy_label_decompress[:,:,0],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 6)\n        plt.imshow(dummy_label_decompress[:,:,1],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 7)\n        plt.imshow(dummy_label_decompress[:,:,2],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 8)\n        plt.imshow(dummy_label_decompress[:,:,3],cmap='gray') # BGR->RGB for plotting\n        plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create a Keras data generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.image as mpimg\n\ndirs_all = os.listdir(TRAIN_PATH)\ndirs_all.sort()\ndirs_train = dirs_all[:2]\ntraining_data = []\nlabel_data = []\n\n\nimport keras\nimport os\nimport sys\nimport nibabel as nib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, CSVLogger\nfrom keras.optimizers import Adam, SGD\nfrom keras import backend as K\n\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels, batch_size=32, dim=(240,240,4),dim_lab=(240,240,1), n_channels=4,\n                 n_classes=2, shuffle=False):\n        'Initialization'\n        self.dim = dim\n        self.dim_lab = dim_lab        \n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n\n        #self.image_size = image_size #Thats new\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        #print(indexes)\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim))\n        y = np.empty((self.batch_size, *self.dim_lab))\n        #print(X.shape)\n        #print(y.shape)\n        # Generate data\n        \n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            #X[i,] = np.load(STORE_PATH + \"/\" + ID)\n            #print(ID)\n            ID_val = np.load(ID)\n            X[i,] = ID_val['arr_0']\n            LABEL_ID = ID.replace(\"Data\", \"Label\")\n            # Store class\n            #y[i,] = np.load(LABEL_PATH + \"/\" + ID)\n            LABEL_ID_val = np.load(LABEL_ID)\n            y[i,] = LABEL_ID_val['arr_0']\n\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dirs_all = [];\ntrain=[];\nenable_original_axial = True\nenable_original_sagittal = True\nenable_original_coronal = True\n\n\n# Parameters\nparams = {'dim': (240,240,4),\n          'dim_lab': (240,240,4),\n          'batch_size': 48,\n          'n_classes': 4,\n          'n_channels': 4,\n          'shuffle': True}\n\n\n\ntrain = np.array(dirs_axial_data_training)\nprint(train.shape)\nif(enable_original_sagittal):    \n    train=np.concatenate((train,np.array(dirs_sagittal_data_training)),axis=0)\nprint(train.shape)\nif(enable_original_coronal): \n    train=np.concatenate((train,np.array(dirs_coronal_data_training)),axis=0)\nvalidation = np.array(dirs_axial_data_validation)\nprint(train.shape)\nprint(validation.shape)\n\ntest = np.array(dirs_axial_data_testing)\n# Generators\n#np.random.shuffle(validation)\n#np.random.shuffle(train)\n\n\ntraining_generator = DataGenerator(train, train, **params)\n\n#print(training_generator)\nvalidation_generator = DataGenerator(validation, validation, **params)\n\ntesting_generator = DataGenerator(test, test, **params)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Show some samples of generated signal:"},{"metadata":{"trusted":true},"cell_type":"code","source":"if(False):\n    x,y = training_generator.__getitem__(0);\n\n    print(x.shape)\n    print(y.shape)\n    num = 0\n    for num in range(32):\n        plt.figure(figsize=(15,8))\n        plt.subplot(2, 4, 1)\n        plt.imshow(x[num,:,:,0],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 2)\n        plt.imshow(x[num,:,:,1],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 3)\n        plt.imshow(x[num,:,:,2],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 4)\n        plt.imshow(x[num,:,:,3],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 5)\n        plt.imshow(y[num,:,:,0],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 6)\n        plt.imshow(y[num,:,:,1],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 7)\n        plt.imshow(y[num,:,:,2],cmap='gray') # BGR->RGB for plotting\n        plt.subplot(2, 4, 8)\n        plt.imshow(y[num,:,:,3],cmap='gray') # BGR->RGB for plotting\n        plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the Deep Learning Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_multi(y_true, y_pred, smooth=1.0):\n    ''' Dice Coefficient\n    Args:\n        y_true (np.array): Ground Truth Heatmap (Label)\n        y_pred (np.array): Prediction Heatmap\n    '''\n\n    class_num = 4\n    for i in range(class_num):\n        y_true_f = K.flatten(y_true[:,:,:,i])\n        y_pred_f = K.flatten(y_pred[:,:,:,i])\n        intersection = K.sum(y_true_f * y_pred_f)\n        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n        if i == 0:\n            total_loss = loss\n        else:\n            total_loss = total_loss + loss\n    total_loss = total_loss / class_num\n    return total_loss\n\ndef dice_coef_whole_tumor(y_true, y_pred, smooth=1.0):\n    ''' Dice Coefficient\n    Args:\n        y_true (np.array): Ground Truth Heatmap (Label)\n        y_pred (np.array): Prediction Heatmap\n    '''\n    y_true_f = y_true[:,:,:,1] + y_true[:,:,:,2] + y_true[:,:,:,3]\n    y_pred_f = y_pred[:,:,:,1] + y_pred[:,:,:,2] + y_pred[:,:,:,3]\n\n    y_true_f = K.flatten(y_true_f)\n    y_pred_f = K.flatten(y_pred_f)\n    intersection = K.sum(y_true_f * y_pred_f)\n    loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n\n    return loss\n\ndef dice_coef_tumor_core(y_true, y_pred, smooth=1.0):\n    ''' Dice Coefficient\n    Args:\n        y_true (np.array): Ground Truth Heatmap (Label)\n        y_pred (np.array): Prediction Heatmap\n    '''\n\n    y_true_f = y_true[:,:,:,1] + y_true[:,:,:,3] \n    y_pred_f = y_pred[:,:,:,1] + y_pred[:,:,:,3] \n\n    y_true_f = K.flatten(y_true_f)\n    y_pred_f = K.flatten(y_pred_f)\n    intersection = K.sum(y_true_f * y_pred_f)\n    loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n\n    return loss\n\n\ndef dice_coef_enhancing_tumor(y_true, y_pred, smooth=1.0):\n    ''' Dice Coefficient\n    Args:\n        y_true (np.array): Ground Truth Heatmap (Label)\n        y_pred (np.array): Prediction Heatmap\n    '''\n\n    y_true_f = y_true[:,:,:,3] \n    y_pred_f = y_pred[:,:,:,3] \n\n    y_true_f = K.flatten(y_true_f)\n    y_pred_f = K.flatten(y_pred_f)\n    intersection = K.sum(y_true_f * y_pred_f)\n    loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n\n    return loss\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1 - dice_coef_multi(y_true, y_pred)\n\ndef UNet_multiclass():\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(inputs)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv1)\n    conv1 =  BatchNormalization(axis=3)(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(pool1)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv2)\n    conv2 =  BatchNormalization(axis=3)(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(pool2)\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv3)\n    conv3 =  BatchNormalization(axis=3)(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(pool3)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv4)\n    conv4 =  BatchNormalization(axis=3)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(pool4)\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv5)\n\n    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(up6)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv6)\n\n    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(up7)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv7)\n\n    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(up8)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv8)\n\n    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(up9)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv9)\n\n    conv10 = Conv2D(4, (1, 1), activation='softmax', padding = 'same')(conv9)\n\n    model = Model(inputs=[inputs], outputs=[conv10])\n    \n    return model\n\ndef UNet_multiclass_regular():\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(inputs)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv1)\n    conv1 =  BatchNormalization(axis=3)(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(pool1)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv2)\n    conv2 =  BatchNormalization(axis=3)(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    pool2 = Dropout(0.5)(pool2)\n\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(pool2)\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv3)\n    conv3 =  BatchNormalization(axis=3)(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    pool3 = Dropout(0.5)(pool3)\n\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(pool3)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv4)\n    conv4 =  BatchNormalization(axis=3)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n    pool4 = Dropout(0.5)(pool4)\n\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(pool4)\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv5)\n\n    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n    up6 = Dropout(0.5)(up6)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(up6)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv6)\n\n    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n    up7 = Dropout(0.5)(up7)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(up7)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv7)\n\n    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n    up8 = Dropout(0.5)(up8)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(up8)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv8)\n\n    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n    up9 = Dropout(0.5)(up9)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(up9)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same',kernel_initializer=initializers.random_normal(stddev=0.01))(conv9)\n\n    conv10 = Conv2D(4, (1, 1), activation='softmax', padding = 'same')(conv9)\n\n    model = Model(inputs=[inputs], outputs=[conv10])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = UNet_multiclass()\nmodel.compile(optimizer=Adam(lr=1e-4), loss=dice_coef_loss, metrics=[dice_coef_multi , dice_coef_whole_tumor, dice_coef_tumor_core , dice_coef_enhancing_tumor])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\ncsv_logger = CSVLogger('my_model.csv')\n\nearlystopper = EarlyStopping(patience=20, verbose=1)\ncheckpointer = ModelCheckpoint('model_BRATS12.h5', verbose=1, save_best_only=True)\n\nhistory = model.fit_generator(generator=training_generator,validation_data=validation_generator, epochs=5, \n                    callbacks=[earlystopper, checkpointer, tensorboard, csv_logger])\nfolder_directory = os.path.join('../working/BraTs_results')\nif not os.path.exists(folder_directory):\n    os.makedirs(folder_directory)\n    model.save('../working/BraTs_results/my_model.h5')\n    model.save_weights('../working/BraTs_results/my_checkpoint')\nelse:\n    model.save('../working/BraTs_results/my_model.h5')\n    model.save_weights('../working/BraTs_results/my_checkpoint')\n\"\"\" ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#folder_directory = os.path.join('../kaggle/BraTs_results')\n#if not os.path.exists(folder_directory):\n#        os.makedirs(folder_directory)\nweight_path_prev = '/kaggle/input/brats-model-weights/model_BRATS13.h5'\nweight_path_last = '../working/model_BRATS13.h5'\nweight_path_best = '../working/model_BRATS13_best.h5'\n\nearlystopper = EarlyStopping(patience=20, verbose=1)\ncheckpointer = ModelCheckpoint(weight_path_best, verbose=1, save_best_only=True)#this saves the best weights with its architecture\nif not os.path.isfile(weight_path_prev): #if the weight doesn't exist, start from scratch\n    print(\"Starting training from the scratch:\")\n    csv_path = '../working/my_model13.csv'\n    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n    csv_logger = CSVLogger(csv_path)\n\n    earlystopper = EarlyStopping(patience=20, verbose=1)\n    checkpointer = ModelCheckpoint(weight_path_best, verbose=1, save_best_only=True)#this saves the best weights with its architecture\n    \n    history = model.fit_generator(generator=training_generator,validation_data=validation_generator, epochs=15, \n                        callbacks=[earlystopper, checkpointer, tensorboard, csv_logger])\n    \n    model.save(weight_path_last) #this saves the last weights with its architecture\n    model.save_weights('../working/my_checkpoint')\n\nelse: #otherwise load the model and continue training\n    from shutil import copyfile\n    print(\"Starting from the perevious weights:\")\n    csv_path = '../working/my_model13.csv'\n    copyfile('/kaggle/input/brats-model-weights/my_model13.csv' , csv_path)\n    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n    csv_logger = CSVLogger(csv_path)\n\n    earlystopper = EarlyStopping(patience=20, verbose=1)\n    checkpointer = ModelCheckpoint(weight_path_best, verbose=1, save_best_only=True)#this saves the best weights with its architecture\n\n    model = load_model(weight_path_prev,custom_objects={'dice_coef_loss':\ndice_coef_loss,'dice_coef_multi': dice_coef_multi,'dice_coef_whole_tumor': dice_coef_whole_tumor,'dice_coef_tumor_core': dice_coef_tumor_core,'dice_coef_enhancing_tumor': dice_coef_enhancing_tumor})\n    history = model.fit_generator(generator=training_generator,validation_data=validation_generator, epochs=15, \n                        callbacks=[earlystopper, checkpointer, tensorboard, csv_logger])\n\n    model.save(weight_path_last) #this saves the last weights with its architecture\n    model.save_weights('../working/my_checkpoint')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.path.isfile('/kaggle/input/brats-model-weights/model_BRATS12.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# summarize history for accuracy\nplt.plot(history.history['dice_coef_multi'])\nplt.plot(history.history['val_dice_coef_multi'])\nplt.title('model dice_coef_multi accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# summarize history for accuracy\nplt.plot(history.history['dice_coef_whole_tumor'])\nplt.plot(history.history['val_dice_coef_whole_tumor'])\nplt.title('model dice_coef_whole_tumor accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# summarize history for accuracy\nplt.plot(history.history['dice_coef_tumor_core'])\nplt.plot(history.history['val_dice_coef_tumor_core'])\nplt.title('model dice_coef_tumor_core accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# summarize history for accuracy\nplt.plot(history.history['dice_coef_enhancing_tumor'])\nplt.plot(history.history['val_dice_coef_enhancing_tumor'])\nplt.title('model dice_coef_enhancing_tumor accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model = load_model('../working/model_BRATS13.h5', custom_objects={'dice_coef_loss':\ndice_coef_loss,'dice_coef_multi': dice_coef_multi,'dice_coef_whole_tumor': dice_coef_whole_tumor,'dice_coef_tumor_core': dice_coef_tumor_core,'dice_coef_enhancing_tumor': dice_coef_enhancing_tumor})\nx,y = validation_generator.__getitem__(0);\ntest_data = x[:,:,:,:]\npred_value = trained_model.predict(test_data, verbose = 1)\nfor num in range(32):\n    print(num)\n    plt.figure(figsize=(15,8))\n    plt.subplot(3, 4, 1)\n    plt.imshow(pred_value[num,:,:,0],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 2)\n    plt.imshow(pred_value[num,:,:,1],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 3)\n    plt.imshow(pred_value[num,:,:,2],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 4)\n    plt.imshow(pred_value[num,:,:,3],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 5)\n    plt.imshow(y[num,:,:,0],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 6)\n    plt.imshow(y[num,:,:,1],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 7)\n    plt.imshow(y[num,:,:,2],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 8)\n    plt.imshow(y[num,:,:,3],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 9)\n    plt.imshow(x[num,:,:,0],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 10)\n    plt.imshow(x[num,:,:,1],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 11)\n    plt.imshow(x[num,:,:,2],cmap='gray') # BGR->RGB for plotting\n    plt.subplot(3, 4, 12)\n    plt.imshow(x[num,:,:,3],cmap='gray') # BGR->RGB for plotting\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.image as mpimg\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\nimport numpy.ma as ma\n\ntrained_model = load_model('../working/model_BRATS13.h5', custom_objects={'dice_coef_loss':\ndice_coef_loss,'dice_coef_multi': dice_coef_multi,'dice_coef_whole_tumor': dice_coef_whole_tumor,'dice_coef_tumor_core': dice_coef_tumor_core\n                                                                        ,'dice_coef_enhancing_tumor': dice_coef_enhancing_tumor})\n\ntest_patients = ['Brats18_2013_22_1','Brats18_CBICA_ABN_1','Brats18_TCIA01_201_1','Brats18_TCIA08_406_1'\n                ,'Brats18_CBICA_ARZ_1','Brats18_TCIA01_131_1','Brats18_2013_7_1','Brats18_2013_0_1'\n                ,'Brats18_2013_6_1','Brats18_TCIA10_266_1']\ncount = 0\ndice_test = 0\nfor nf, folder in enumerate(dirs_train):\n  print(\"Processing Images...\")\n  HLG_PATH = os.path.join(TRAIN_PATH,folder)\n  hlg_all = os.listdir(HLG_PATH)  \n\n  for n, file in enumerate(hlg_all):\n    if file in test_patients:\n        count = count + 1\n        print(file)\n        print(\"Image: \"+ str(nf) + \", \" + str(n))\n        flair = nib.load(os.path.join(HLG_PATH, file, file + '_flair.nii.gz')).get_data()\n        t1 = nib.load(os.path.join(HLG_PATH, file, file + '_t1.nii.gz')).get_data()\n        t1ce = nib.load(os.path.join(HLG_PATH, file, file + '_t1ce.nii.gz')).get_data()\n        t2 = nib.load(os.path.join(HLG_PATH, file, file + '_t2.nii.gz')).get_data()\n        label = nib.load(os.path.join(HLG_PATH, file, file + '_seg.nii.gz')).get_data()\n\n        print(\"Axial Data:\")    \n        axial = 0;\n        for i in range(155):\n            if(np.mean(flair[:,:,i]+t1[:,:,i]+t1ce[:,:,i]+t2[:,:,i])!=0):\n                data_patch_axial = np.zeros([240,240,4],dtype='float32')\n                data_patch_axial[:,:,0] = normalize_image(flair[:,:,i])\n                data_patch_axial[:,:,1] = normalize_image(t1[:,:,i])\n                data_patch_axial[:,:,2] = normalize_image(t1ce[:,:,i])\n                data_patch_axial[:,:,3] = normalize_image(t2[:,:,i])\n                if (axial == 0):\n                    data_patch_axial_test = np.expand_dims(data_patch_axial,axis=0)\n                else:\n                    data_patch_axial = np.expand_dims(data_patch_axial,axis=0)\n                    data_patch_axial_test = np.concatenate((data_patch_axial_test,data_patch_axial),axis = 0)\n                axial = axial + 1\n                \n        axial = 0\n        for i in range(155):\n            if(np.mean(flair[:,:,i]+t1[:,:,i]+t1ce[:,:,i]+t2[:,:,i])!=0):\n                label_patch_axial = np.zeros([240,240,4],dtype='int')\n                label_patch_axial[:,:,0] = (label[:,:,i]==0).astype(int)\n                label_patch_axial[:,:,1] = (label[:,:,i]==1).astype(int)\n                label_patch_axial[:,:,2] = (label[:,:,i]==2).astype(int)\n                label_patch_axial[:,:,3] = (label[:,:,i]==4).astype(int)\n                if (axial == 0):\n                    label_patch_axial_test = np.expand_dims(label_patch_axial,axis=0)\n                else:\n                    label_patch_axial = np.expand_dims(label_patch_axial,axis=0)\n                    label_patch_axial_test=np.concatenate((label_patch_axial_test,label_patch_axial),axis = 0)\n                axial = axial + 1\n         \n        print(label_patch_axial_test.shape[0])\n        print(data_patch_axial_test.shape)\n        pred_value = trained_model.predict(data_patch_axial_test, verbose = 1)\n        evaluation = trained_model.evaluate(x=data_patch_axial_test, y=label_patch_axial_test,batch_size = label_patch_axial_test.shape[0]\n                                               )\n        print(evaluation)\n        dice_test = np.asarray(evaluation) +dice_test\n\nprint(\"test accuracy:\")\nprint(dice_test/count)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}